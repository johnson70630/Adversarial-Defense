# Adversarial-Defense
This project was part of the AI course's poster session, focusing on the purification of attacked images. Image attacks leading to AI misinterpretations can pose life-threatening situations in various applications such as drug identification and autonomous driving. Collaborating with three engineering-background peers, we explored the Deepfool and FGSM models to attack images. Additionally, we utilized the Diffusion model and APE-GAN to cleanse the attacked images. Ultimately, we found that the U-net within the diffusion model proved highly effective in image purification, elevating the accuracy of identifying attacked images from 2% to 95%.

## [poster session video](https://www.youtube.com/watch?v=1pyfJ1MkRGo&t=57s)

## [public code & poster](https://drive.google.com/drive/folders/1fkCfwQb_DT3DPH6-osgNwtI0ifQ6pasz)
